---
template: SolutionPage
title: Infrastructure and HPC
subtitle: Secure and scalable in-house and cloud-based high-performance-computing (HPC)
relslug: high-performance-computing
featured:
  caption: Shane checking on his IG
  image: /assets/PittockBuild-20170218-21.jpg
categories:
  - category: Cyberinfrastructure
---
As the size and complexity of data collection and modeling efforts grow, the need for scientists to routinely access HPC resources is becoming more common. Whether you have 30 years of model results that you want to process into monthly climatologies, or you want to query a database with a few million rows of ship locations, we can help. Axiom Data Science builds and provides access to HPC compute clusters that can make these types of services available fast and over the web.

Axiom maintains a High Performance Compute (HPC) cluster in Portland, Oregon at the Pittock. Our HPC resources are composed of more than 5000 processing cores staged in a series of interconnected blade arrays as well as over 3 PBs of storage distributed across clustered storage nodes. Compute nodes and storage nodes are connected over a low latency, converging network fabric (40 Gb/Sec Infiniband). GlusterFS is employed as a storage software abstraction layer that enables clients and storage servers to exploit data transfer over Remote Direct Memory Access (RDMA) protocols. This configuration enables data throughput from the storage cluster to the compute cluster to reach speeds greater than 160 Gb/Sec in high-concurrency situations necessary for large-scale scientific modeling as well as hosting, querying and visualizing large, long-term data sets. Axiom also has a dedicated multi-braided 1 Gb/Sec high speed internet connection for large file transfers between external data centers and the high-bandwidth demands of centralized web/cloud based applications.
